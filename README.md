# Visual-InCompatibility-Transformer
Repository for the "VICTOR: Visual Incompatibility Detection with Transformers and Fashion-specific contrastive pre-training" paper. The pre-print can be read here: https://arxiv.org/abs/2207.13458

## Abstract
>*In order to consider fashion outfits as aesthetically pleasing, the garments that constitute them need to be compatible in terms of visual aspects, such as style, category and color. With the advent and omnipresence of computer vision deep learning models, increased interest has also emerged for the task of visual compatibility detection with the aim to develop quality fashion outfit recommendation systems. Previous works have defined visual compatibility as a binary classification task with items in a garment being considered as fully compatible or fully incompatible. However, this is not applicable to Outfit Maker applications where users create their own outfits and need to know which specific items may be incompatible with the rest of the outfit. To address this, we propose the Visual InCompatibility TransfORmer (VICTOR) that is optimized for two tasks: 1) overall compatibility as regression and 2) the detection of mismatching items. Unlike previous works that either rely on feature extraction from ImageNet-pretrained models or by end-to-end fine tuning, we utilize fashion-specific contrastive language-image pre-training for fine tuning computer vision neural networks on fashion imagery. Moreover, we build upon the Polyvore outfit benchmark to generate partially mismatching outfits, creating a new dataset termed Polyvore-MISFITs, that is used to train VICTOR. A series of ablation and comparative analyses show that the proposed architecture can compete and even surpass the current state-of-the-art on Polyvore datasets while reducing the instance-wise floating operations by 88%, striking a balance between high performance and efficiency.*

## Usage
- The Polyvore dataset can be downloaded from https://github.com/mvasil/fashion-compatibility. 
- Define `data_path` in main.py as the directory that you selected to store the Polyvore dataset.
- Create a python (> 3.8) environment (Venv or Anaconda) and install the `requirements.txt`. Moreover, you should follow the instruction provided in https://github.com/openai/CLIP to install CLIP.
- Run `python main.py` to first train the four FLIP computer vision models, extract the visual and textual features and then run the ablation and comparative experiments found in the paper. 
